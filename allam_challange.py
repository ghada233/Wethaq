# -*- coding: utf-8 -*-
"""Allam Challange.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQ1ghaKm1r5cBm0mNBjR2_OTUXxEmye2
"""

# input -----> pic ---> label "name "  ------> ""----> SPEECH AUDIO
# Portal present FAST-API
import numpy as np
import matplotlib.pyplot as plt
import glob
import cv2

from keras.models import Model, Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
import os
import seaborn as sns
from keras.applications.vgg16 import VGG16
from keras.applications import DenseNet201
import lightgbm as lgb

SIZE = 224  #Resize images

#Capture training data and labels into respective lists
train_images = []
train_labels = []

from google.colab import drive
drive.mount('/content/drive')

for directory_path in glob.glob("/content/drive/MyDrive/allam/Train/*"): #Train Path
    label = directory_path.split("/")[-1]
    print(label)
    for img_path in glob.glob(os.path.join(directory_path, "*.*")):
        print(img_path)
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)
        img = cv2.resize(img, (SIZE, SIZE))
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        train_images.append(img)
        train_labels.append(label)

#Convert lists to arrays
train_images = np.array(train_images)
train_labels = np.array(train_labels)


# Capture test/validation data and labels into respective lists

test_images = []
test_labels = []
for directory_path in glob.glob("/content/drive/MyDrive/allam/Test/*"): #Test Path
    fruit_label = directory_path.split("/")[-1]
    for img_path in glob.glob(os.path.join(directory_path, "*.*")):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)
        img = cv2.resize(img, (SIZE, SIZE))
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        test_images.append(img)
        test_labels.append(fruit_label)


#Convert lists to arrays
test_images = np.array(test_images)
test_labels = np.array(test_labels)


#Encode labels from text to integers.
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(test_labels)
test_labels_encoded = le.transform(test_labels)
le.fit(train_labels)
train_labels_encoded = le.transform(train_labels)


#Rename (already split but assigning to meaningful convention)
x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded
x_train, y_train, x_test, y_test

# Normalize
x_train, x_test = x_train / 255.0, x_test / 255.0


#One hot encode y values for neural network.
from keras.utils import to_categorical
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)


#Load model without FC layers
VGG_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(SIZE, SIZE, 3))

#Set trainable to false
for layer in VGG_model.layers:
	layer.trainable = False

VGG_model.summary()

#Use DL to extract features
feature_extractor=VGG_model.predict(x_train)

features = feature_extractor.reshape(feature_extractor.shape[0], -1)

X_for_training = features #input to RF

from sklearn.linear_model._stochastic_gradient import SGDClassifier

model = SGDClassifier()

model.fit(X_for_training, y_train)

#For testing
X_test_feature = VGG_model.predict(x_test)
X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)


#Prediction
prediction = model.predict(X_test_features)
#Inverse le transform
prediction = le.inverse_transform(prediction)


print(prediction)

!pip install joblib
VGG_model.save('VGG_model.h5')
joblib.dump(model, 'random_forest_model.pkl')  # Save with model
joblib.dump(le, 'le_model.pkl')  # Save with le

#Test model
img = cv2.imread("/content/59191.jpg", cv2.IMREAD_COLOR)
plt.imshow(img)
img = cv2.resize(img, (SIZE, SIZE))
img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
img = img/255.0
input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)
input_img_feature=VGG_model.predict(input_img)
input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)
prediction_RF = model.predict(input_img_features)[0]
prediction_RF = le.inverse_transform([prediction_RF])  #Reverse the label encoder to original name
print("The prediction for this image is: ", prediction_RF)